{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct our finalised dataset, we had to wrangle and combine data from numerous sources:\n",
    "- A set of provided CSV data that contain tweets posted by @WeRateDogs.\n",
    "- A set of provided TSV data that contains image classification results for a selection of tweets from our primary dataset. Although we did not have access to the classifier itself, we were able to obtain 3 predictions for a set of tweet ids. These predictions indicated the confidence of the prediction and a True/False value for whether any of the images were classified as a dog or not a dog. \n",
    "- Extended tweet data pulled directly from the Twitter API. This data was first saved line by line into a text file, before being read into a pandas dataframe.\n",
    "\n",
    "We cleaned both data quality and structure issues before combining our datasets to a single master data frame, which was exported to CSV. Specifically, we:\n",
    "- reduced the column count by unpivoting dog stage columns which could more efficiently be captured as rows. We parsed the text again to make sure that any doggo's, puppers etc weren't missed. \n",
    "- corrected the name, rating_numerator and rating_denominator fields by parsing the text field again, including correctly identifying and recording decimal numerators for the rating. \n",
    "- removed records where we couldnt provide complete data (tweets, favourite count, retweet count and image predictions) across all the datsets. For the images dataset, this meant removing records where none of the predictions were identified as a dog. For the extended twitter dataset, we excluded records from the merge, if we could not join on a `tweet_id`. \n",
    "- manually corrected a few records that had incorrect ratings and dog stages.\n",
    "- corrected timestamp column time so that it would be easier to perform data calculations using this field\n",
    "- converted most of the id-related columns to `object` types and made sure the rating numerator and donominators were float.\n",
    "- We identified and removed records that were retweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finalised the cleaning process by creating a single, exported csv file, `twitter_archive_master.csv` that could be used for further analysis. This master dataset was then read into pandas as a final dataframe, from which the analysis was completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
